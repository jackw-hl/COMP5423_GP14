{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de92456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets faiss-cpu huggingface_hub pytrec_eval tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8564d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import InferenceClient\n",
    "import pytrec_eval\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840a6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12000\n",
      "Validation: 1500\n",
      "Test: 1052\n",
      "Collection: 144718\n"
     ]
    }
   ],
   "source": [
    "# Load HQ-small dataset\n",
    "dataset = load_dataset(\"izhx/COMP5423-25Fall-HQ-small\")\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"validation\"]\n",
    "test_ds = dataset[\"test\"]\n",
    "coll_ds = dataset[\"collection\"]\n",
    "\n",
    "print(\"Train:\", len(train_ds))\n",
    "print(\"Validation:\", len(val_ds))\n",
    "print(\"Test:\", len(test_ds))\n",
    "print(\"Collection:\", len(coll_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a82845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE_DIR: retrieval_model/dense_instruction_e5-mistral\n",
      "Expecting files:\n",
      "  retrieval_model/dense_instruction_e5-mistral/doc_embs.npy\n",
      "  retrieval_model/dense_instruction_e5-mistral/faiss_index.bin\n",
      "  retrieval_model/dense_instruction_e5-mistral/doc_ids.json\n"
     ]
    }
   ],
   "source": [
    "# Change this to your own folder (on Colab or local)\n",
    "BASE_DIR = \"retrieval_model\"  # or any path you used\n",
    "MODEL_TAG = \"dense_instruction_e5-mistral\"          # just a folder name to group stuff\n",
    "\n",
    "SAVE_DIR = os.path.join(BASE_DIR, MODEL_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "EMB_PATH = os.path.join(SAVE_DIR, \"doc_embs.npy\")\n",
    "INDEX_PATH = os.path.join(SAVE_DIR, \"faiss_index.bin\")\n",
    "IDS_PATH = os.path.join(SAVE_DIR, \"doc_ids.json\")\n",
    "\n",
    "print(\"SAVE_DIR:\", SAVE_DIR)\n",
    "print(\"Expecting files:\")\n",
    "print(\" \", EMB_PATH)\n",
    "print(\" \", INDEX_PATH)\n",
    "print(\" \", IDS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2cd913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_embs shape: (144718, 4096)\n",
      "FAISS index vectors: 144718\n",
      "Number of doc_ids: 144718\n"
     ]
    }
   ],
   "source": [
    "# Load precomputed doc embeddings + FAISS index + doc_ids\n",
    "\n",
    "if not (os.path.exists(EMB_PATH) and os.path.exists(INDEX_PATH) and os.path.exists(IDS_PATH)):\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find one of the required files: doc_embs.npy, faiss_index.bin, doc_ids.json.\\n\"\n",
    "        \"Make sure you set SAVE_DIR correctly and that you already built the index.\"\n",
    "    )\n",
    "\n",
    "doc_embs = np.load(EMB_PATH)\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "with open(IDS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    doc_ids = json.load(f)\n",
    "\n",
    "print(\"doc_embs shape:\", doc_embs.shape)\n",
    "print(\"FAISS index vectors:\", index.ntotal)\n",
    "print(\"Number of doc_ids:\", len(doc_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f79a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF InferenceClient initialized.\n"
     ]
    }
   ],
   "source": [
    "# Option 1: set it here (for quick testing)\n",
    "HF_TOKEN = \"hf_XXXXXXXXXXXXXXXXXXXXXXXX\"  # <-- put your token here\n",
    "\n",
    "# Option 2 (recommended): set environment variable instead, then:\n",
    "# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if HF_TOKEN is None or HF_TOKEN == \"\":\n",
    "    raise ValueError(\"HF_TOKEN is not set. Please set your Hugging Face token.\")\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"auto\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"HF InferenceClient initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23dcc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_e5_mistral_embed(texts):\n",
    "    \"\"\"\n",
    "    texts: str or list[str]\n",
    "    returns: np.ndarray (B, dim)\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    # ✅ no 'inputs=' keyword, just pass texts as first arg\n",
    "    outputs = client.feature_extraction(\n",
    "        texts,\n",
    "        model=\"intfloat/e5-mistral-7b-instruct\",\n",
    "    )\n",
    "\n",
    "    arr = np.array(outputs, dtype=\"float32\")\n",
    "    if arr.ndim == 1:   # single vector\n",
    "        arr = arr[None, :]\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74cff4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_DESCRIPTION = (\n",
    "    \"Given a multi-hop question about Wikipedia, retrieve the most relevant passages \"\n",
    "    \"that help answer the question.\"\n",
    ")\n",
    "\n",
    "def format_e5_instruction_query(q: str) -> str:\n",
    "    return f\"Instruct: {TASK_DESCRIPTION}\\nQuery: {q}\"\n",
    "\n",
    "def encode_queries_via_hf_api(questions):\n",
    "    \"\"\"\n",
    "    questions: list[str]\n",
    "    returns: np.ndarray (B, dim), L2-normalized\n",
    "    \"\"\"\n",
    "    texts = [format_e5_instruction_query(q) for q in questions]\n",
    "    embs = hf_e5_mistral_embed(texts)\n",
    "\n",
    "    # L2 norm\n",
    "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "    embs = embs / np.clip(norms, 1e-12, None)\n",
    "\n",
    "    return embs.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47e848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding shape: (1, 4096)\n",
      "Matches doc_emb dim: True\n"
     ]
    }
   ],
   "source": [
    "test_emb = encode_queries_via_hf_api([\"Who is the president of the United States?\"])\n",
    "print(\"Query embedding shape:\", test_emb.shape)\n",
    "print(\"Matches doc_emb dim:\", test_emb.shape[1] == doc_embs.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df85ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(question: str, k: int = 10):\n",
    "    \"\"\"\n",
    "    Runs retrieval for a single question using:\n",
    "      - HF API to embed query\n",
    "      - local FAISS index built on your doc_embs\n",
    "    \"\"\"\n",
    "    q_emb = encode_queries_via_hf_api([question])  # (1, dim)\n",
    "    scores, idx = index.search(q_emb, k)          # FAISS search\n",
    "\n",
    "    scores = scores[0]\n",
    "    idx = idx[0]\n",
    "\n",
    "    results = [(doc_ids[i], float(scores[j])) for j, i in enumerate(idx)]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b53b087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The second place finisher of the 2011 Gran Premio Santander d'Italia drove for who when he won the 2009 FIA Formula One World Championship?\n",
      "Supporting IDs: ['doc-23954', 'doc-109746']\n",
      "\n",
      "Top-10 retrieved:\n",
      "✔ doc-23954 | 0.8087\n",
      "  doc-59626 | 0.7761\n",
      "  doc-45965 | 0.7748\n",
      "  doc-43480 | 0.7693\n",
      "✔ doc-109746 | 0.7670\n",
      "  doc-100598 | 0.7670\n",
      "  doc-52698 | 0.7648\n",
      "  doc-43038 | 0.7594\n",
      "  doc-45341 | 0.7473\n",
      "  doc-121143 | 0.7426\n"
     ]
    }
   ],
   "source": [
    "example_item = val_ds[0]\n",
    "print(\"Question:\", example_item[\"text\"])\n",
    "print(\"Supporting IDs:\", example_item[\"supporting_ids\"])\n",
    "\n",
    "hits = retrieve(example_item[\"text\"], k=10)\n",
    "print(\"\\nTop-10 retrieved:\")\n",
    "for doc_id, score in hits:\n",
    "    mark = \"✔\" if doc_id in example_item[\"supporting_ids\"] else \" \"\n",
    "    print(f\"{mark} {doc_id} | {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b77a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_for_split(split_ds, output_path, top_k=10):\n",
    "    \"\"\"\n",
    "    split_ds: HF dataset (validation or test)\n",
    "    Writes JSONL with fields:\n",
    "      - id\n",
    "      - question\n",
    "      - answer ('' for test)\n",
    "      - retrieved_docs: [[doc_id, score], ...]\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in tqdm(split_ds, desc=\"Retrieving via HF API\"):\n",
    "            qid = item[\"id\"]\n",
    "            question = item[\"text\"]\n",
    "            answer = item.get(\"answer\", \"\")  # test doesn't have answer\n",
    "\n",
    "            hits = retrieve(question, k=top_k)\n",
    "\n",
    "            rec = {\n",
    "                \"id\": qid,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"retrieved_docs\": [\n",
    "                    [doc_id, float(score)] for doc_id, score in hits\n",
    "                ],\n",
    "            }\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\"Saved predictions to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d4ee552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving via HF API:   2%|▏         | 33/1500 [00:33<24:32,  1.00s/it]\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/nebius/v1/embeddings (Request ID: Root=1-692bc70c-20d402062e47ca3d384c5c05;11c64671-85ff-41b8-830d-afd0d65a9a18)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/COMP5423_GP14/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/COMP5423_GP14/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/nebius/v1/embeddings",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m VAL_OUTPUT_PATH = os.path.join(\n\u001b[32m      2\u001b[39m     SAVE_DIR, \u001b[33m\"\u001b[39m\u001b[33mvalidation_dense_instruction_e5_mistral.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mwrite_predictions_for_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVAL_OUTPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mwrite_predictions_for_split\u001b[39m\u001b[34m(split_ds, output_path, top_k)\u001b[39m\n\u001b[32m     13\u001b[39m question = item[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     14\u001b[39m answer = item.get(\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# test doesn't have answer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m hits = \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m rec = {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: qid,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     ],\n\u001b[32m     25\u001b[39m }\n\u001b[32m     26\u001b[39m f.write(json.dumps(rec, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mretrieve\u001b[39m\u001b[34m(question, k)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m = \u001b[32m10\u001b[39m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Runs retrieval for a single question using:\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m      - HF API to embed query\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m      - local FAISS index built on your doc_embs\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     q_emb = \u001b[43mencode_queries_via_hf_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1, dim)\u001b[39;00m\n\u001b[32m      8\u001b[39m     scores, idx = index.search(q_emb, k)          \u001b[38;5;66;03m# FAISS search\u001b[39;00m\n\u001b[32m     10\u001b[39m     scores = scores[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mencode_queries_via_hf_api\u001b[39m\u001b[34m(questions)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03mquestions: list[str]\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03mreturns: np.ndarray (B, dim), L2-normalized\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m texts = [format_e5_instruction_query(q) \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m embs = \u001b[43mhf_e5_mistral_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# L2 norm\u001b[39;00m\n\u001b[32m     18\u001b[39m norms = np.linalg.norm(embs, axis=\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mhf_e5_mistral_embed\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m      7\u001b[39m     texts = [texts]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ✅ no 'inputs=' keyword, just pass texts as first arg\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m outputs = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mintfloat/e5-mistral-7b-instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m arr = np.array(outputs, dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr.ndim == \u001b[32m1\u001b[39m:   \u001b[38;5;66;03m# single vector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/COMP5423_GP14/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:1078\u001b[39m, in \u001b[36mInferenceClient.feature_extraction\u001b[39m\u001b[34m(self, text, normalize, prompt_name, truncate, truncation_direction, model)\u001b[39m\n\u001b[32m   1065\u001b[39m provider_helper = get_provider_helper(\u001b[38;5;28mself\u001b[39m.provider, task=\u001b[33m\"\u001b[39m\u001b[33mfeature-extraction\u001b[39m\u001b[33m\"\u001b[39m, model=model_id)\n\u001b[32m   1066\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m   1067\u001b[39m     inputs=text,\n\u001b[32m   1068\u001b[39m     parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m   1076\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m   1077\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1079\u001b[39m np = _import_numpy()\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(provider_helper.get_response(response), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/COMP5423_GP14/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:275\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/COMP5423_GP14/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:475\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/nebius/v1/embeddings (Request ID: Root=1-692bc70c-20d402062e47ca3d384c5c05;11c64671-85ff-41b8-830d-afd0d65a9a18)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "VAL_OUTPUT_PATH = os.path.join(\n",
    "    SAVE_DIR, \"validation_dense_instruction_e5_mistral.jsonl\"\n",
    ")\n",
    "\n",
    "write_predictions_for_split(val_ds, VAL_OUTPUT_PATH, top_k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
